% !TeX encoding = UTF-8
% !TeX program = pdflatex

\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[USenglish,british,american,australian,english]{babel}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\lstset{%
	basicstyle=\fontsize{10}{11}\ttfamily\color{black},
	commentstyle=\ttfamily\color{red},
	keywordstyle=\ttfamily\color{blue},
	stringstyle=\color{orange},
	tabsize=2,
	numbers=left,
	numberstyle=\tiny,
	firstnumber=1,
	numberfirstline=false,
	frame=single,
	showstringspaces=false,
	inputencoding=utf8,
	breaklines=true,
	language=python,
}

\title{\textbf{Compiler Provenance} \\ \bigskip \large Homework 1 - Machine Learning \\ Engineering in Computer Science \\ "La Sapienza" University of Rome}
\author{Costa Marco 1691388}
\date{\today}

\begin{document}
\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
The purpose of this homework is to apply machine learning algorithms to solve the compiler provenance problem.
It consists of identifying the \textbf{compiler} and the \textbf{optimization} who produced a given binary code of a function.
So we have two types of classification problems:
\begin{itemize}
	\item \textbf{Binary classification}: optimization can be \textit{HIGH (H)} or \textit{LOW (L)}
	\item \textbf{Multi-class classification}: compiler can be \textit{gcc}, \textit{icc} or \textit{clang}
\end{itemize}

\section{Dataset}
The dataset contains 30000 functions
compiled with 3 different compilers (\textit{gcc}, \textit{icc}, \textit{clang}).
The compiler distribution is very balanced
(10000 functions per compiler), while the optimizations distribution is not balanced. For each compiler different versions were used.
The Dataset is provided as a jsonl file.
Each row of the file is a json object with the
following keys:
\begin{itemize}
\item \textbf{instructions}: the assembly instruction for the
function.
\item \textbf{opt}: the ground truth label for optimization (\textit{H}, \textit{L})
\item \textbf{compiler}: the ground truth label for compiler (\textit{icc},
\textit{clang}, \textit{gcc})
\end{itemize}
Moreover, there is a blind test set, which does not contain the label for the function. The goal is to classify the functions of this dataset after training the algorithm on the previous set.

\section{Find best classifier}
To find the algorithm that best solves the problem I decided to analyze several cases, using different types of features, classifiers and vectorizers.

\subsection{Feature}
I decided to use the Bag-of-Words approach. It consists in:
\begin{itemize}
	\item Splitting the documents into tokens by following some sort of pattern. In this case, for each function there is a string composed of instructions separated by a comma, so it's possible to split it on the comma.
	\item Assigning a weight to each token proportional to the frequency with which it shows up in the document and/or corpora.
	\item Creating a document-term matrix with each row representing a document and each column addressing a token.
\end{itemize}
I used two types of different approaches. The first consists of using only the mnemonics instructions (as suggested in the paper), while the second using all the complete instructions. I also tried to manipulate the instructions, but that lowered the accuracy a lot.

\subsection{Vectorizer}
I used two vectorizer objects provided by Scikit-Learn. They allow us to perform all the above steps at once efficiently, and even apply preprocessing and rules regarding the number and frequency of tokens.

\begin{itemize}
	\item \textbf{Count Vectorizer}: It counts the number of times a token shows up in the document and uses this value as its weight.
	\item \textbf{TF-IDF Vectorizer}: TF-IDF stands for “term frequency-inverse document frequency”, meaning the weight assigned to each token not only depends on its frequency in a document but also how recurrent that term is in the entire corpora.
\end{itemize}

Since the order of the instructions is important, I also used the vectorizers with the \textit{ngram\_range} parameter in order to look for ngram relationships at multiple scales, setting it once to (3,3) and once to (4,4)

\subsection{How to choose the best classifier}
A good model is not the one that gives accurate predictions on the known data or training data but the one which gives good predictions on the new data and avoids overfitting and underfitting. \\
In order to choose the classifier that performs better on the dataset, it's possible to analyze and compare some parameters:
\begin{itemize}
	\item \textbf{Accuracy} - is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations
	\item \textbf{Precision} - is the ratio of correctly predicted positive observations to the total predicted positive observations
	\item \textbf{Recall} - is the ratio of correctly predicted positive observations to the all observations in actual class - yes
	\item \textbf{F1 score} - is the weighted average of Precision and Recall ($F1 Score = 2*(Recall * Precision) / (Recall + Precision)$)
	\item \textbf{K Fold Cross Validation} - to tackle problem of overfitting it's possible to perform the \textit{Cross Validation}. It consists of partitioning the data into a number of subsets, hold out a set at a time and train the model on remaining set and test model on hold out set, repeating the process for each subset of the dataset.
\end{itemize}

\subsection{Classifier}
Different classifiers were used:
\begin{itemize}
	\item \textbf{Logistic Regression}
	\item \textbf{Multinomial Naive Bayes}
	\item \textbf{Decision Tree}
	\item \textbf{SVC} with \textit{rbf} kernel
	\item \textbf{K-Neighbors}
	\item \textbf{Random Forest}
\end{itemize}

\subsection{Result}
Using the \textbf{\textit{mnemonic instructions}} as feature, the \textit{SVC} classifier performed better for both optimization and compiler classification, using the \textit{Tfidf vectorizer with ngram range (3,3)}
\begin{itemize}
	\item \textbf{Optimization classification}: \\
	\begin{center}
		\textbf{\textit{Confusion matrix}} $= \begin{bmatrix}
		2019 & 421 \\
		337 & 3223
		\end{bmatrix}$\\
		\textbf{\textit{Accuracy}} $= 0.874$\\
		\textbf{\textit{Precision}} $= 0.871$\\
		\textbf{\textit{Recall}} $= 0.866$\\
		\textbf{\textit{Cross validation}} $= \begin{bmatrix}
		0.822 & 0.85 & 0.825 & 0.827 & 0.825 & 0.857 & 0.858 & 0.845 & 0.84 & 0.822
		\end{bmatrix}$\\
		\textbf{\textit{F1 score}} $= 0.868$
	\end{center}
	
	\item \textbf{Compiler classification}: \\
	\begin{center}
		\textbf{\textit{Confusion matrix}} $= \begin{bmatrix}
		1773 & 112 & 76 \\
		114 & 1800 & 95 \\
		107 & 69 & 1854
		\end{bmatrix}$\\
		\textbf{\textit{Accuracy}} $= 0.905$\\
		\textbf{\textit{Precision}} $= 0.904$\\
		\textbf{\textit{Recall}} $= 0.904$\\
		\textbf{\textit{Cross validation}} $= \begin{bmatrix}
		0.862 & 0.853 & 0.863 & 0.868 & 0.843 & 0.837 & 0.84 & 0.832 & 0.868 & 0.863
		\end{bmatrix}$\\
		\textbf{\textit{F1 score}} $= 0.904$
	\end{center}
\end{itemize}

Using the \textbf{\textit{complete instructions}}, instead, the \textit{SVC} classifier performed better with the \textit{Tfidf} vectorizer without the \textit{ngram\_range} for both optimization and compiler classification.
\begin{itemize}
	\item \textbf{Optimization classification}: \\
	\begin{center}
		\textbf{\textit{Confusion matrix}} $= \begin{bmatrix}
		1982 & 427 \\
		445 & 3146
		\end{bmatrix}$\\
		\textbf{\textit{Accuracy}} $= 0.855$\\
		\textbf{\textit{Precision}} $= 0.849$\\
		\textbf{\textit{Recall}} $= 0.849$\\
		\textbf{\textit{Cross validation}} $= \begin{bmatrix}
		0.802 & 0.805 & 0.81 & 0.822 & 0.823 & 0.807 & 0.805 & 0.815 & 0.817 & 0.838
		\end{bmatrix}$\\
		\textbf{\textit{F1 score}} $= 0.849$
	\end{center}
	
	\item \textbf{Compiler classification}: \\
	\begin{center}
		\textbf{\textit{Confusion matrix}} $= \begin{bmatrix}
		1832 & 77 & 71 \\
		125 & 1822 & 65 \\
		82 & 66 & 1860
		\end{bmatrix}$\\
		\textbf{\textit{Accuracy}} $= 0.919$\\
		\textbf{\textit{Precision}} $= 0.919$\\
		\textbf{\textit{Recall}} $= 0.919$\\
		\textbf{\textit{Cross validation}} $= \begin{bmatrix}
		0.872 & 0.857 & 0.863 & 0.842 & 0.857 & 0.87 & 0.873 & 0.86 & 0.86 & 0.906
		\end{bmatrix}$\\
		\textbf{\textit{F1 score}} $= 0.919$
	\end{center}
\end{itemize}
Previous results are the best results obtained. For completeness, the results of all the cases tested are attached in the "\textit{scores}" folder.

\section{Predictions}
From the tests carried out it can be seen that the best classifier is \textit{SVC} with the \textit{mnemonic instructions} and the \textit{Tfidf} vectorizer with \textit{ngram\_range = (3,3)}  for the optimization classification, while \textit{SVC} with the \textit{complete instructions} without \textit{ngram\_range} for the compiler classification. Since the difference between the use of mnemonic and complete instructions is not so great in the compiler classification, for simplicity the prediction on the \textit{blind test set} has been made using the mnemonic instructions. \\
Results:
\begin{itemize}
	\item \textbf{Optimization classification}: 1793 functions with \textbf{\textit{L}} optimization, 1207 functions with \textbf{\textit{H}} optimization
	\item \textbf{Compiler classification}: 975 functions compiled with \textbf{\textit{gcc}}, 1013 functions compiled with \textbf{\textit{icc}}, 1012 functions compiled with \textbf{\textit{clang}}
\end{itemize}
We can say that the compiler distribution is balanced, while the second is not (as in the train dataset). \\
A csv file is also attached, in which for each instruction the prediction of the compiler and the optimization is given.

\section{Code}
Two scripts have been implemented in Python 3. The first allows you to test different classifiers, vectorizers and features in order to obtain useful parameters to compare the various cases and choose the best algorithm. The second one allows to classify the blind test set instructions according to the compiler (Multi-class Classification) and the optimization (Binary Classification), using the desired classifier, vectorizer and feature.

\subsection{Usage}
\begin{itemize}
	\item \textbf{\textit{find\_best\_classifier.py}}: python3 find\_best\_classifier.py classification\_type instruction\_type
	\item \textbf{\textit{predict.py}}: \textit{python3 predict.py opt\_instruction\_type opt\_classifier opt\_vectorizer comp\_instruction\_type comp\_classifier comp\_vectorizer}
\end{itemize}
You need to specify some parameters:
\begin{itemize}
	\item \textit{classification\_type}: "Optimization"/"Compiler", to choose whether to classify by optimization or by compiler 
	\item \textit{instruction\_type}: "mnemonics"/"instructions", to choose whether to use only mnemonic or complete instructions
	\item \textit{opt\_instructions}: "mnemonics"/"instructions", to choose whether to use only mnemonic or complete instructions for the optimization classification
	\item \textit{opt\_classifier}: an integer between 0 and 5, to choose the classifier for the optimization classification
	\item \textit{opt\_vectorizer}: an integer between 0 and 5, to choose the vectorizer for the optimization classification
	\item \textit{comp\_instructions}: "mnemonics"/"instructions", to choose whether to use only mnemonic or complete instructions for the compiler classification
	\item \textit{comp\_classifier}: an integer between 0 and 5, to choose the classifier for the compiler classification
	\item \textit{comp\_vectorizer}: an integer between 0 and 5, to choose the vectorizer for the compiler classification
\end{itemize}

The possible classifiers are:
\begin{itemize}
	\item \textit{0} - Logistic Regression
	\item \textit{1} - Multinomial NB
	\item \textit{2} - Decision Tree
	\item \textit{3} - SVC
	\item \textit{4} - K-Neighbors
	\item \textit{5} - Random Forest
\end{itemize}

The possible vectorizers are:
\begin{itemize}
	\item \textit{0} - Count vectorizer
	\item \textit{1} - Tfidf vectorizer
	\item \textit{2} - Count vectorizer with ngram\_range=(3,3)
	\item \textit{3} - Tfidf vectorizer with ngram\_range=(3,3)
	\item \textit{4} - Count vectorizer with ngram\_range=(4,4)
	\item \textit{5} - Tfidf vectorizer with ngram\_range=(4,4)
\end{itemize}

\subsection{Code explanation}
Let's analyze some important functions.\\ \\
\textbf{Read the dataset}:
\begin{lstlisting}
def read_data():
	print("Reading data ...")
	dataset = []
	with open(train_dataset_path, 'r') as json_file:
		json_list = list(json_file)
	
	for json_str in json_list:
		result = json.loads(json_str)
		instructions = result["instructions"]
		instr = []
		mnemonics = []
		for instruction in instructions:
			instr.append(instruction)
			mnemonics.append(instruction.split(" ")[0])
		result["instructions"] = ",".join(instr)
		result["mnemonics"] = ",".join(mnemonics)
	dataset.append(result)
	
	data = pd.DataFrame(dataset)
	print("Data read successfully!")
	print("-"*50)
	print(data.shape)
	print("-"*50)
	return data
\end{lstlisting}
This function allows you to read the dataset in jsonl format. Once opened, the instructions of each function of the dataset are manipulated in order to obtain two strings (one for the mnemonic instructions and one for the complete instructions. The whole is returned as a pandas dataframe. \\

\textbf{Split the dataset}:
\begin{lstlisting}
def split_dataset(classifier_type, instruction_type):
	target = ""
	if classifier_type == "Optimization":
		target = "opt"
	else:
		target = "compiler"
	x_train, x_test, y_train, y_test = train_test_split(data[instruction_type], data[target], test_size = 0.20)
	encoder = preprocessing.LabelEncoder()
	y_train = encoder.fit_transform(y_train)
	y_test = encoder.fit_transform(y_test)
	return x_train, x_test, y_train, y_test
\end{lstlisting}
This function allows you to divide the dataset according to the type of classification required (Optimization or Compiler). 80\% of the dataset is used for model training and 20\% for testing. In addition, the target is encoded. \\

\textbf{Classify}:
\begin{lstlisting}
def classify(x_train_count, x_test_count, y_train, y_test, vectorizer, instruction_type, classifier_type):
	for classifier in classifiers:
		print("="*50, file = score_file)
		print("Classifier:",type(classifier).__name__,"[Vectorizer: " + vectorizer +"], [Instruction type: " + instruction_type +"], [Classifier type: " + classifier_type + "]", file = score_file)
		print("="*50, file = score_file)
		print("Fitting",type(classifier).__name__,"[Vectorizer: " + vectorizer +"], [Instruction type: " + instruction_type +"], [Classifier type: " + classifier_type + "]")
		classifier.fit(x_train_count, y_train)
		print("-"*50)
		print("Predicting",type(classifier).__name__,"[Vectorizer: " + vectorizer +"], [Instruction type: " + instruction_type +"], [Classifier type: " + classifier_type + "]")
		y_pred = classifier.predict(x_test_count)
		print("-"*50)
		print("Result:", file = score_file)
		print("-"*50, file = score_file)
		print("Confusion Matrix:", file = score_file)
		print(confusion_matrix(y_test, y_pred), file = score_file)
		print("-"*50, file = score_file)
		print("Classification Report:", file = score_file)
		print(classification_report(y_test, y_pred), file = score_file)
		print("-"*50, file = score_file)
		print("Accuracy:", file = score_file)
		print(accuracy_score(y_test, y_pred), file = score_file)
		print("-"*50, file = score_file)
		print("Precision:", file = score_file)
		print(precision_score(y_test, y_pred, average="macro"), file = score_file)
		print("-"*50, file = score_file)
		print("Recall:", file = score_file)
		print(recall_score(y_test, y_pred, average="macro"), file = score_file)
		print("-"*50, file = score_file)
		print("Performing Cross Validation Score",type(classifier).__name__,"[Vectorizer: " + vectorizer +"], [Instruction type: " + instruction_type +"], [Classifier type: " + classifier_type + "]")
		print("Cross Validation Score:", file = score_file)
		print(cross_val_score(classifier, x_test_count, y_test, cv = 10, n_jobs = -1), file = score_file)
		print("-"*50)
		print("-"*50, file = score_file)
		print("F1 Score:", file = score_file)
		print(f1_score(y_test, y_pred, average="macro"), file = score_file)
		print("-"*50, file = score_file)
\end{lstlisting}
This last function allows you to train and test the six models used for this experiment. After obtaining the various parameters for each model, these are written into a \textit{file.txt}, so that they can be analyzed.

\section{Conclusions}
From the results obtained it is possible to conclude that \textit{SVC} is the classifier that performs better to solve the compiler provenance problem, both in the case of the mnemonic instructions and in the case of the complete instructions. In addition, it can be noted that considering the complete instructions does not significantly improve the results, so it is advisable to always use the mnemonic instructions. Another consideration is that the order in which the instructions appear is important, and in general consider blocks of 3 instructions to be more successful than considering blocks of 4 instructions.

\end{document}